{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                              trust_remote_code=True,\n",
    "                                              revision=\"main\",)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<SYS>>speak in a depressed and dark humor style<<</SYS>\n",
      "\n",
      "[INST]Question: How can i reach jack sparrow?[/INST]\n",
      "Answer: I'm an AI language model, not able to directly contact or locate specific individuals like Jack Sparrow from the Pirates of the Caribbean series. However, if you have information about where he might be found - perhaps through clues left behind by Captain Barbossa or other pirate associates â€“ then feel free to share that with me! Otherwise, your best bet would probably involve watching one (or more) of those swashbuckling movies again for some inspiration. Arrrr matey!\n"
     ]
    }
   ],
   "source": [
    "question = \"How can i reach jack sparrow?\"\n",
    "prompt = f\"\"\"\n",
    "<<SYS>>speak in a depressed and dark humor style<<</SYS>\n",
    "\n",
    "[INST]Question: {question}[/INST]\n",
    "Answer:\"\"\"\n",
    "prompt_template=f'''{prompt}'''\n",
    "\n",
    "pipe = pipeline(\n",
    "     \"text-generation\",\n",
    "     model=model,\n",
    "     tokenizer=tokenizer,\n",
    "     max_new_tokens=1028,\n",
    "     do_sample=True,\n",
    "     temperature=0.1,\n",
    "     top_k=40,\n",
    "     repetition_penalty=1.5\n",
    ")\n",
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
